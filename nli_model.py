# -*- coding: utf-8 -*-
"""NLI_model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EbzbP_dEYmqpFA7uxIVZNkkZntEKxXAu
"""

import torch.nn as nn
import torch.nn.functional as F
import torch
class MLP(nn.Module):

  def __init__(self, out_dim,inp_dim,drop_prob):
    super(MLP, self).__init__()
    self.fc1=nn.Linear(inp_dim,inp_dim//2)
    self.fc2=nn.Linear(inp_dim//2,inp_dim//2)
    self.fc3=nn.Linear(inp_dim//2,inp_dim//4)
    self.fc4=nn.Linear(inp_dim//4,out_dim)
    self.drop=nn.Dropout(drop_prob)

  def forward(self, x):
    x=F.relu(self.fc1(x))
    x=self.drop(x)
    x=F.relu(self.fc2(x))
    x=self.drop(x)
    x=F.relu(self.fc3(x))
    x=self.drop(x)
    x=self.fc4(x)
    return x
  '''x=F.dropout(F.relu(self.fc1(x)),p=self.p)
      x=F.dropout(F.relu(self.fc2(x)),p=self.p)
      x=F.dropout(F.relu(self.fc3(x)),p=self.p)
      x=self.fc4(x)
      return x'''

class My_LSTM(nn.Module):

  def __init__(self,n_layer,embedding_features,hidden_features,drop_p):
      super(My_LSTM, self).__init__()
      self.hidden_features=hidden_features
      self.n_layer=n_layer
      self.rnn = nn.LSTM(input_size=embedding_features, hidden_size=hidden_features,
                      num_layers=n_layer, dropout=drop_p,bidirectional=True)

  def forward(self, inputs):
      batch_size = inputs.size()[1]
      state_shape = 2*self.n_layer, batch_size,self.hidden_features
      h0 = c0 = inputs.new_zeros(state_shape)
      outputs, (ht, ct) = self.rnn(inputs, (h0, c0))
      return ht[-2:].transpose(0, 1).contiguous().view(batch_size, -1)

class SnliClassifier(nn.Module):

  def __init__(self,vocalLen):
    super(SnliClassifier,self).__init__()
    #print(out_dim)
    #n_RNN_layer=2
    hidden_features=256
    embedding_features=300
    out_dim=4
    lstm_layers=2
    drop_prob1=0.2
    drop_prob2=0.2
    self.embedding=nn.Embedding(vocalLen,embedding_features)
    self.RNN=My_LSTM(lstm_layers,embedding_features,hidden_features,drop_prob1)
    self.clf=MLP(out_dim,4*hidden_features,drop_prob2)
    
  def forward(self,batch):
    sen1 = self.embedding(batch.premise)
    sen2 = self.embedding(batch.hypothesis)
    premise = self.RNN(sen1)
    hypothesis = self.RNN(sen2)
    out = self.clf(torch.cat([premise, hypothesis], 1))
    return out

model=SnliClassifier(50)

