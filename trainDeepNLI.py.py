# -*- coding: utf-8 -*-
"""NLI_DeepTrain.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IxsYRyoU81beVPNVV_6jqLJXaVwv3aoa
"""

import pandas as pd
import numpy as np
import torch
import torchtext
from torchtext.data import Field, BucketIterator
import torch.nn as nn
import torch.nn.functional as F
from tqdm import tqdm
import nli_model as nli

from torchtext import data
from torchtext import datasets

import nltk
from nltk.stem import WordNetLemmatizer
import re
nltk.download('wordnet')
wordnet=WordNetLemmatizer()
#stopwordset=set(stopwords.words('english'))
def tokenize(string):
  review=re.sub('[^a-zA-Z]',' ',string)
  #review=review.lower()
  review=review.split()
  review=[wordnet.lemmatize(word) for word in review]
  return review
sentences = data.Field(lower=True, tokenize=tokenize)
ans = data.Field(sequential=False)

train, dev, test = datasets.SNLI.splits(sentences, ans)


sentences.build_vocab(train, dev, test,min_freq=3)
ans.build_vocab(train, dev, test)
if torch.cuda.is_available():
    device = torch.device('cuda:0')
else:
    device = torch.device('cpu')
print('device',device)    
Batch_Size=64
train_iter, dev_iter, test_iter = data.BucketIterator.splits(
            (train, dev, test), batch_size=Batch_Size, device=device)

def train(model,train_loader,val_loader,optimizer,criterion,scheduler,epochs,print_iter=5):
    val_loss=[]
    train_loss=[]
    acc_list=[]
    max_acc=0
    #bestModel='hello'
    for i in tqdm(range(epochs)):
        model.train()
        train_loader.init_epoch()
        for indx,inputs in enumerate(train_loader):
            optimizer.zero_grad()
            output=model(inputs)
            loss=criterion(output,inputs.label)
            loss.backward()
            optimizer.step()
        if (i%print_iter)==0:
            model.eval()
            running_corrects=0.0
            running_loss=0.0
            total=0.0
            with torch.no_grad():
                for inputs in val_loader:
                    optimizer.zero_grad()
                    output=model(inputs)
                    loss=criterion(output,inputs.label)
                    _,pred=torch.max(output, 1)
                    running_corrects += torch.sum(pred == inputs.label).item()
                    running_loss+=loss.item()
                    total+=inputs.batch_size
                valLoss=running_loss/total
                acc=(running_corrects/total)
                val_loss.append(valLoss)
                acc_list.append(acc)
                for inputs in train_loader:
                    optimizer.zero_grad()
                    output=model(inputs)
                    loss=criterion(output,inputs.label)
                    running_loss+=loss.item()
                    total+=inputs.batch_size
                trainLoss=running_loss/total
                train_loss.append(trainLoss)
                if(acc>max_acc):
                  torch.save(model.state_dict(), "/content/drive/My Drive/NLI.pt")
                  max_acc=acc
            print(' {} Val Loss: {:.6f} Train Loss: {:.6f} Acc: {:.6f}'.format(
                  i,valLoss,trainLoss,acc))
        scheduler.step()
    return train_loss,val_loss,acc_list

from google.colab import drive
drive.mount('/content/drive')

import torch.optim as optim
model=nli.SnliClassifier(len(sentences.vocab))
lr=0.0005
optimizer=optim.Adam(model.parameters(),lr,weight_decay=0.000)
criterion=nn.CrossEntropyLoss()
#device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
exp_lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=4, gamma=0.1)
model.to(device)

trainLossList,valLossList,accList=train(model,train_iter,dev_iter,optimizer,criterion,exp_lr_scheduler,epochs=10,print_iter=1)

